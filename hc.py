import os
import time
import gc
import queue
import threading
from concurrent.futures import ProcessPoolExecutor, as_completed

import cv2
import numpy as np
import torch

from detect_core.castfilm_detector import CastFilmDefectDetector
from detect_core.defect_classifier import DefectClassifier
from detect_core.defect_config import DefectConfig


# ------------------------
# 1. å…¨å±€æ£€æµ‹å™¨åˆå§‹åŒ–å‡½æ•°ï¼ˆä¾›è¿›ç¨‹æ± å…±äº«ï¼‰
# ------------------------
_detector = None


def _init_detector():
    """åœ¨æ¯ä¸ªå­è¿›ç¨‹é‡Œåˆå§‹åŒ–ä¸€æ¬¡ CastFilm æ£€æµ‹å™¨"""
    global _detector
    _detector = CastFilmDefectDetector()


def imread_unicode(path, flags=cv2.IMREAD_GRAYSCALE):
    """å®‰å…¨è¯»å–å›¾ç‰‡ï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„"""
    try:
        with open(path, "rb") as f:
            file_bytes = np.frombuffer(f.read(), dtype=np.uint8)
            img = cv2.imdecode(file_bytes, flags)
            return img
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶: {os.path.basename(path)}, é”™è¯¯: {e}")
        return None


def _segment_one_image(image_path):
    """
    å­è¿›ç¨‹åˆ†å‰²ä»»åŠ¡ï¼š
    1) è¯»å–ç°åº¦å›¾
    2) è°ƒç”¨ CastFilmDefectDetector.detect_defects_fast
    è¿™é‡Œåªä¿ç•™ç¼ºé™·æ¡†ï¼Œä¸åšä»»ä½•ä¿å­˜/å‰ç«¯äº¤äº’
    """
    try:
        global _detector
        image_np_gray = imread_unicode(image_path, cv2.IMREAD_GRAYSCALE)
        if image_np_gray is None:
            return False, image_path, []

        defects, left_edge_x, right_edge_x = _detector.detect_defects_fast(image_np_gray)
        return True, image_path, defects
    except Exception as e:
        print(f"âš ï¸ åˆ†å‰²é˜¶æ®µå‡ºé”™: {os.path.basename(image_path)}, é”™è¯¯: {e}")
        return False, image_path, []


# ------------------------
# 2. åˆ†ç±» + ç»Ÿè®¡
# ------------------------
def process_dataset(classify_result, count):
    """
    ä»…æ›´æ–°ç»Ÿè®¡è®¡æ•°å™¨ï¼š
    å¯¹äºæ¯ä¸ªç¼ºé™·ï¼š
        - defect_classï¼ˆé»‘ç‚¹/æ™¶ç‚¹/çº¤ç»´/å…¶å®ƒï¼‰
        - size_index â†’ SIZE_LIST åç§°
        - count[defect_class][size_name] += 1
    """
    cut_images = classify_result.get("cut_images", [])
    defect_infos = classify_result.get("defect_infos", [])
    for _, defect_info in zip(cut_images, defect_infos):
        defect_class = defect_info.get("defect_class", "å…¶å®ƒ")
        size_index = defect_info.get("size_index", 0)
        if 0 <= size_index < len(DefectConfig.SIZE_LIST):
            size_name = DefectConfig.SIZE_LIST[size_index]
            # å…œåº•ï¼šåˆ†ç±»å™¨å¦‚æœç»™äº†ä¸åœ¨é¢„è®¾é‡Œçš„ç±»åï¼Œå½’å…¥â€œå…¶å®ƒâ€
            if defect_class not in count:
                defect_class = "å…¶å®ƒ"
            count[defect_class][size_name] += 1


def _safe_qsize(q):
    try:
        return q.qsize()
    except Exception:
        return -1


def classification_worker(result_queue: "queue.Queue", classifier, count, batch_size=6):
    """
    åˆ†ç±»çº¿ç¨‹ï¼š
        - ä»é˜Ÿåˆ— result_queue å– (image_path, defects)
        - è¯»ç°åº¦å›¾ï¼ŒæŒ‰ batch åš classify_defects_batch
        - ç”¨ process_dataset æ›´æ–°ç»Ÿè®¡
    é˜Ÿåˆ—é‡Œæ”¶åˆ° None æ—¶é€€å‡ºã€‚
    """
    batch_images, batch_defects = [], []
    processed_count = 0
    cls_start = time.perf_counter()

    while True:
        item = result_queue.get()  # é˜»å¡ç­‰å¾…
        if item is None:
            break

        image_path, defects = item
        image_np_gray = imread_unicode(image_path, cv2.IMREAD_GRAYSCALE)
        if image_np_gray is None:
            continue

        batch_images.append(image_np_gray)
        batch_defects.append(defects)

        if len(batch_images) >= batch_size:
            t0 = time.perf_counter()
            classify_results = classifier.classify_defects_batch(batch_images, batch_defects)
            t1 = time.perf_counter()

            processed_count += len(batch_images)
            elapsed = t1 - cls_start
            avg_time = elapsed / processed_count if processed_count else 0.0
            speed = processed_count / elapsed if elapsed > 0 else 0.0
            qn = _safe_qsize(result_queue)

            print(
                f"[åˆ†ç±»çº¿ç¨‹] å·²åˆ†ç±» {processed_count} å¼  | "
                f"æœ¬æ‰¹è€—æ—¶ {(t1 - t0):.3f}s | "
                f"å¹³å‡ {avg_time:.3f}s/å¼  | "
                f"é€Ÿåº¦ {speed:.2f} å¼ /ç§’ | "
                f"é˜Ÿåˆ—é•¿åº¦ {qn if qn >= 0 else 'NA'}"
            )

            for result in classify_results:
                process_dataset(result, count)

            batch_images, batch_defects = [], []

    # å¤„ç†å‰©ä½™æœªæ»¡ batch çš„å†…å®¹
    if batch_images:
        classify_results = classifier.classify_defects_batch(batch_images, batch_defects)
        for result in classify_results:
            process_dataset(result, count)


# ------------------------
# 3. å•ä¸ªå­æ–‡ä»¶å¤¹å¤„ç†ï¼šåˆ†å‰² + åˆ†ç±» + ç»Ÿè®¡ï¼ˆæ— å…‰ç…§æ£€æµ‹ï¼‰
# ------------------------
def process_single_subfolder(
    subfolder_path,
    batch_size=6,
    queue_maxsize=200,
    classifier=None,
    executor=None,
):
    """
    å¤„ç†å•ä¸ªå­æ–‡ä»¶å¤¹ï¼š
        1) ä»å­æ–‡ä»¶å¤¹è¯»å–æ‰€æœ‰å›¾åƒï¼ˆåªåˆ†å‰²+åˆ†ç±»ï¼Œä¸ä¿å­˜ï¼‰
        2) ç”¨è¿›ç¨‹æ± åšåˆ†å‰²ï¼›ç”¨åˆ†ç±»çº¿ç¨‹æ‰¹é‡åˆ†ç±»ï¼›æ›´æ–°å°ºå¯¸ç»Ÿè®¡
    è¿”å›ï¼š (å­æ–‡ä»¶å¤¹è·¯å¾„, ç»Ÿè®¡å­—å…¸)
    """
    print(f"\nğŸ“‚ å¼€å§‹å¤„ç†å­æ–‡ä»¶å¤¹ï¼š{os.path.basename(subfolder_path)}")

    IMAGE_EXTS = (".jpg", ".jpeg", ".png", ".bmp", ".tiff")
    image_files = [
        os.path.join(subfolder_path, f)
        for f in sorted(os.listdir(subfolder_path))
        if f.lower().endswith(IMAGE_EXTS)
    ]
    if not image_files:
        print(f"[WARN] æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶ï¼š{os.path.basename(subfolder_path)}")
        return subfolder_path, {}

    # åˆå§‹åŒ–ç»Ÿè®¡ï¼šæ¯ä¸ªç±»åˆ« Ã— æ¯ä¸ªå°ºå¯¸æ®µ
    categories = ["é»‘ç‚¹", "æ™¶ç‚¹", "çº¤ç»´", "å…¶å®ƒ"]
    count = {cat: {size: 0 for size in DefectConfig.SIZE_LIST} for cat in categories}

    # å¯åŠ¨åˆ†ç±»çº¿ç¨‹ï¼ˆåªåšåˆ†ç±»+ç»Ÿè®¡ï¼‰
    result_queue = queue.Queue(maxsize=queue_maxsize)
    cls_thread = threading.Thread(
        target=classification_worker,
        args=(result_queue, classifier, count, batch_size),
        daemon=True,
    )
    cls_thread.start()

    # ---------------- åˆ†å‰²éƒ¨åˆ† ----------------
    print(f"ğŸš€ å¯åŠ¨åˆ†å‰²ä»»åŠ¡ï¼Œå…± {len(image_files)} å¼ ")
    seg_start_all = time.perf_counter()
    completed = 0

    futures = [executor.submit(_segment_one_image, path) for path in image_files]
    for f in as_completed(futures):
        ok, image_path, defects = f.result()
        completed += 1

        if completed % 10 == 0 or completed == len(futures):
            elapsed = time.perf_counter() - seg_start_all
            avg_time = elapsed / completed if completed else 0.0
            print(f"[åˆ†å‰²è¿›åº¦] {completed}/{len(futures)} | {elapsed:.1f}s | {avg_time:.3f}s/å¼ ")

        if not ok:
            continue

        # queue.Queue ä¼šåœ¨æ»¡æ—¶é˜»å¡ï¼Œä¸ç”¨ä½ æ‰‹åŠ¨ sleep è½®è¯¢
        result_queue.put((image_path, defects))

    # é€šçŸ¥åˆ†ç±»çº¿ç¨‹ç»“æŸ
    result_queue.put(None)
    cls_thread.join()

    # æ¸…ç†æ˜¾å­˜ä¸å†…å­˜
    torch.cuda.empty_cache()
    gc.collect()

    # å­æ–‡ä»¶å¤¹ç¼ºé™·æ€»æ•°ï¼ˆå¯æ‰“å°ï¼‰
    sub_total = sum(count[cat][size] for cat in count for size in count[cat])
    print(f"ğŸ“Š å­æ–‡ä»¶å¤¹ç¼ºé™·æ€»æ•°: {sub_total}")
    print(f"âœ… å­æ–‡ä»¶å¤¹å®Œæˆï¼š{os.path.basename(subfolder_path)}")

    return subfolder_path, count


# ------------------------
# 4. å¤šå­æ–‡ä»¶å¤¹æ‰¹é‡å¤„ç† + æ±‡æ€»æŠ¥å‘Šï¼ˆæ— å…‰ç…§æ£€æµ‹ï¼‰
# ------------------------
def process_multi_subfolders(
    root_input_folder,
    root_output_folder,
    batch_size=6,
    max_workers=8,
    queue_maxsize=200,
    report_name_level=1,
    custom_name=None,
):
    """
    å¤šæ–‡ä»¶å¤¹æ‰¹é‡æ£€æµ‹ï¼ˆä»…åˆ†å‰²+åˆ†ç±»+ç»Ÿè®¡ï¼‰ï¼š
        - root_input_folder ä¸‹æ¯ä¸ªå­ç›®å½•ä¸€ä¸ªâ€œæ‰¹æ¬¡â€
        - ä¸ä¿å­˜å›¾åƒï¼Œä¸ä¸å‰ç«¯äº¤äº’ï¼Œåªå†™ä¸€ä¸ªæ±‡æ€» txt æŠ¥å‘Š
    """
    print(f"ğŸ” å¯åŠ¨å¤šæ–‡ä»¶å¤¹æ‰¹é‡æ£€æµ‹: {root_input_folder}")
    if not os.path.exists(root_input_folder):
        print(f"[ERROR] è¾“å…¥è·¯å¾„ä¸å­˜åœ¨ï¼š{root_input_folder}")
        return

    os.makedirs(root_output_folder, exist_ok=True)

    # åŠ¨æ€æŠ¥å‘Šå‘½åé€»è¾‘
    path_parts = os.path.normpath(root_input_folder).split(os.sep)
    if custom_name:
        folder_name = custom_name
    elif len(path_parts) >= report_name_level:
        folder_name = path_parts[-report_name_level]
    else:
        folder_name = os.path.basename(os.path.normpath(root_input_folder))

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    report_filename = f"{folder_name}_summary_{timestamp}.txt"
    report_path = os.path.join(root_output_folder, report_filename)
    print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šå°†ä¿å­˜ä¸º: {report_filename}")

    subfolders = [
        os.path.join(root_input_folder, d)
        for d in os.listdir(root_input_folder)
        if os.path.isdir(os.path.join(root_input_folder, d))
    ]
    if not subfolders:
        print(f"[ERROR] æ²¡æœ‰å­æ–‡ä»¶å¤¹")
        return

    # å¤–å±‚åªåˆå§‹åŒ–ä¸€æ¬¡åˆ†ç±»å™¨å’Œè¿›ç¨‹æ± 
    defect_classifier = DefectClassifier()

    results = []
    with ProcessPoolExecutor(max_workers=max_workers, initializer=_init_detector) as executor:
        for subfolder in subfolders:
            res = process_single_subfolder(
                subfolder_path=subfolder,
                batch_size=batch_size,
                queue_maxsize=queue_maxsize,
                classifier=defect_classifier,
                executor=executor,
            )
            results.append(res)

    # è®¡ç®—æ€»ç¼ºé™·æ•°
    grand_total = 0
    for _, count in results:
        if not count:
            continue
        grand_total += sum(count[cat][size] for cat in count for size in count[cat])

    # å†™å‡ºæ±‡æ€»æŠ¥å‘Šï¼ˆçº¯æ–‡æœ¬ï¼Œä¸ä¿å­˜ä»»ä½•å›¾åƒï¼‰
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("å¤šæ–‡ä»¶å¤¹ç¼ºé™·æ£€æµ‹æ±‡æ€»æŠ¥å‘Šï¼ˆæ— å…‰ç…§æ£€æµ‹ï¼‰\n")
        f.write(f"ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"è¾“å…¥æ ¹ç›®å½•ï¼š{root_input_folder}\n")
        f.write("=" * 80 + "\n\n")

        for subfolder_path, count in results:
            f.write(f"æ–‡ä»¶å¤¹: {os.path.basename(subfolder_path)}\n")
            if count:
                sub_total = sum(count[cat][size] for cat in count for size in count[cat])
                f.write(f"ç¼ºé™·æ€»æ•°: {sub_total}\n")
                f.write("class_name " + " ".join(DefectConfig.SIZE_LIST) + "\n")
                for cat in ["é»‘ç‚¹", "æ™¶ç‚¹", "çº¤ç»´", "å…¶å®ƒ"]:
                    f.write(
                        f"{cat} "
                        + " ".join(str(count[cat][s]) for s in DefectConfig.SIZE_LIST)
                        + "\n"
                    )
            else:
                f.write("ç¼ºé™·ç»Ÿè®¡: æ— ï¼ˆè¯¥æ–‡ä»¶å¤¹æ— å›¾åƒæˆ–å¤„ç†å¤±è´¥ï¼‰\n")
            f.write("-" * 80 + "\n\n")

        f.write(f"æ‰€æœ‰å­æ–‡ä»¶å¤¹æ€»ç¼ºé™·æ•°é‡: {grand_total}\n")

    print(f"ğŸ¯ æ‰€æœ‰å­æ–‡ä»¶å¤¹æ€»ç¼ºé™·æ•°é‡ï¼š{grand_total}")
    print(f"ğŸ‰ æ‰€æœ‰å­æ–‡ä»¶å¤¹å¤„ç†å®Œæˆï¼æŠ¥å‘Šä¿å­˜è‡³ {report_path}")


if __name__ == "__main__":
    # ğŸ‘‡ æŒ‰éœ€ä¿®æ”¹ä½ çš„è¾“å…¥/è¾“å‡ºè·¯å¾„
    ROOT_INPUT_FOLDER = r"G:\NEW\20250815\data2"
    ROOT_OUTPUT_FOLDER = "./output_summary_reports"

    process_multi_subfolders(
        root_input_folder=ROOT_INPUT_FOLDER,
        root_output_folder=ROOT_OUTPUT_FOLDER,
        batch_size=4,
        max_workers=8,
        queue_maxsize=512,
        report_name_level=2,       # ä¾‹å¦‚ç”¨å€’æ•°ç¬¬äºŒå±‚å‘½å
        # custom_name="film_batchA"
    )
